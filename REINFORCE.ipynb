{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE algorithm\n",
    "The following code presents an agent using the REINFORCE algorithm and solving the short corridor problem in the Barton Sutto book, Reinforcement Learning: An Introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2018 Sergii Bondariev (sergeybondarev@gmail.com)                    #\n",
    "# 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ShortCorridor:\n",
    "    \"\"\"\n",
    "    Short corridor environment, see Example 13.1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "\n",
    "    def step(self, go_right):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            go_right (bool): chosen action\n",
    "        Returns:\n",
    "            tuple of (reward, episode terminated?)\n",
    "        \"\"\"\n",
    "        if self.state == 0 or self.state == 2:\n",
    "            if go_right:\n",
    "                self.state += 1\n",
    "            else:\n",
    "                self.state = max(0, self.state - 1)\n",
    "        else:\n",
    "            if go_right:\n",
    "                self.state -= 1\n",
    "            else:\n",
    "                self.state += 1\n",
    "\n",
    "        if self.state == 3:\n",
    "            # terminal state\n",
    "            return 0, True\n",
    "        else:\n",
    "            return -1, False\n",
    "\n",
    "class ReinforceAgent:\n",
    "    \"\"\"\n",
    "    ReinforceAgent that follows algorithm\n",
    "    'REINFORCE Monte-Carlo Policy-Gradient Control (episodic)'\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, gamma):\n",
    "        # set values such that initial conditions correspond to left-epsilon greedy\n",
    "        self.theta = np.array([-1.47, 1.47])\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        # first column - left, second - right\n",
    "        self.x = np.array([[0, 1],\n",
    "                           [1, 0]])\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "    def get_pi(self):\n",
    "        h = np.dot(self.theta, self.x)\n",
    "        t = np.exp(h - np.max(h))\n",
    "        pmf = t / np.sum(t)\n",
    "        # never become deterministic,\n",
    "        # guarantees episode finish\n",
    "        imin = np.argmin(pmf)\n",
    "        epsilon = 0.05\n",
    "\n",
    "        if pmf[imin] < epsilon:\n",
    "            pmf[:] = 1 - epsilon\n",
    "            pmf[imin] = epsilon\n",
    "\n",
    "        return pmf\n",
    "\n",
    "    def get_p_right(self):\n",
    "        \"\"\"Gets probability of moving right\"\"\"\n",
    "        return self.get_pi()[1]\n",
    "\n",
    "    def choose_action(self, reward):\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "        \n",
    "        # Get policy\n",
    "        pmf = self.get_pi()\n",
    "        go_right = np.random.uniform() <= pmf[1]\n",
    "        self.actions.append(go_right)\n",
    "\n",
    "        return go_right\n",
    "\n",
    "    def episode_end(self, last_reward):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.rewards.append(last_reward)\n",
    "\n",
    "        # learn theta\n",
    "        G = np.zeros(len(self.rewards))\n",
    "        G[-1] = self.rewards[-1]\n",
    "\n",
    "        for i in range(2, len(G) + 1):\n",
    "            G[-i] = self.gamma * G[-i + 1] + self.rewards[-i]\n",
    "\n",
    "        gamma_pow = 1\n",
    "\n",
    "        for i in range(len(G)):\n",
    "            # Retrieve action from time step\n",
    "            j = 1 if self.actions[i] else 0\n",
    "            pmf = self.get_pi()\n",
    "            \n",
    "            # Calculate eligibility vector\n",
    "            grad_ln_pi = self.x[:, j] - np.dot(self.x, pmf)\n",
    "            \n",
    "            # Calculate updates for parameters.\n",
    "            update = self.alpha * gamma_pow * G[i] * grad_ln_pi\n",
    "            \n",
    "            # Update theta accordingly using the gradient, alpha, gamma and G.\n",
    "            self.theta += update\n",
    "            # Update gamma_pow as we move further into the future where \n",
    "            # rewards matter less.\n",
    "            gamma_pow *= self.gamma\n",
    "        \n",
    "        # Clean records so the next episode can start\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "\n",
    "\n",
    "def figure_13_1():\n",
    "    num_trials = 100\n",
    "    num_episodes = 1000\n",
    "    gamma = 1\n",
    "    agent_generators = [lambda : ReinforceAgent(alpha=2e-4, gamma=gamma),\n",
    "                        lambda : ReinforceAgent(alpha=2e-2, gamma=gamma),\n",
    "                        lambda : ReinforceAgent(alpha=2e-3, gamma=gamma)]\n",
    "    labels = ['alpha = 2e-4',\n",
    "              'alpha = 2e-5',\n",
    "              'alpha = 2e-3']\n",
    "\n",
    "    rewards = np.zeros((len(agent_generators), num_trials, num_episodes))\n",
    "\n",
    "    for agent_index, agent_generator in enumerate(agent_generators):\n",
    "        for i in tqdm(range(num_trials)):\n",
    "            reward = trial(num_episodes, agent_generator)\n",
    "            rewards[agent_index, i, :] = reward\n",
    "\n",
    "    plt.plot(np.arange(num_episodes) + 1, -11.6 * np.ones(num_episodes), ls='dashed', color='red', label='-11.6')\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.plot(np.arange(num_episodes) + 1, rewards[i].mean(axis=0), label=label)\n",
    "    plt.ylabel('total reward on episode')\n",
    "    plt.xlabel('episode')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE agents' rewards\n",
    "This figure shows the different total rewards to which each agent approaches to as they participate in more and more episodes. It is important to notice how the one approaching the actual optimal value is the medium sized step. This highlights the importance of choosing a good step size so that our agent doesn't jump over the correct answer but is also fast at converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:31<00:00,  1.10it/s]\n",
      "100%|██████████| 100/100 [07:30<00:00,  4.50s/it]\n",
      " 92%|█████████▏| 92/100 [03:41<00:12,  1.59s/it]"
     ]
    }
   ],
   "source": [
    "figure_13_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
